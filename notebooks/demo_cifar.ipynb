{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 109 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pprint\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "MAIN_PATH = os.getcwd().split(\"notebooks\")[0]\n",
    "sys.path.insert(0, MAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Federated Learning experiment\n",
    "from data_loader.cifar10 import Cifar10DatasetManager\n",
    "from server.base_server import BaseServer\n",
    "from client.base_client import BaseClient\n",
    "from experiments.base_experiment import BaseExperiment\n",
    "from gradients.noise import GaussianNoiseGenerator, NoNoiseGenerator,StaircaseNoiseGenerator\n",
    "from metrics.classification import multiclass_accuracy\n",
    "from models.cifar_model import SimpleCifarCNN, EfficientCifarCNN,ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoCifar10Experiment(BaseExperiment):\n",
    "    def __init__(self, \n",
    "                 client_num: int = 2, \n",
    "                 lr: float = 0.01, \n",
    "                 noise_generator=None,\n",
    "                 max_norm: float = 200,\n",
    "                 sampling_rate: float = 0.05):\n",
    "        if noise_generator is None:\n",
    "            noise_generator = NoNoiseGenerator()\n",
    "        self.noise_generator = noise_generator\n",
    "        self.lr = lr\n",
    "        self.max_norm = max_norm\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.client_num = client_num\n",
    "        self._init_server_clients(client_num, self.lr)\n",
    "        self._init_data(client_num)\n",
    "\n",
    "    def _init_server_clients(self, client_num, lr):\n",
    "        model = ResNet\n",
    "        self.clients = [BaseClient(model(lr=lr, max_norm=self.max_norm), \n",
    "                                   client_id=idx, \n",
    "                                   noise_generator=self.noise_generator)\n",
    "                        for idx in range(client_num)]\n",
    "        self.server = BaseServer(model(lr=lr, max_norm=self.max_norm))\n",
    "\n",
    "    def _init_data(self, client_num):\n",
    "        data_manager = Cifar10DatasetManager(n_parties=client_num, \n",
    "                                             sampling_lot_rate=self.sampling_rate)\n",
    "        self.client_train_datas = data_manager.train_loaders\n",
    "        self.valid_datas = data_manager.validation_loader\n",
    "        self.test_data = data_manager.test_loader\n",
    "\n",
    "    def evaluate_model(self, data):\n",
    "        total_correct = 0\n",
    "        total_sample_num = 0\n",
    "        with torch.no_grad():\n",
    "            for _, (inputs, target) in enumerate(data):\n",
    "                predict_labels = self.server.predict(inputs)\n",
    "                correct, sample_num = multiclass_accuracy(y_pred=predict_labels, \n",
    "                                                          y_true=target)\n",
    "                total_correct += correct\n",
    "                total_sample_num += sample_num\n",
    "                \n",
    "        return total_correct / total_sample_num\n",
    "        \n",
    "    def get_validation_result(self):\n",
    "        return self.evaluate_model(self.valid_datas)\n",
    "    \n",
    "    def get_test_result(self):\n",
    "        return self.evaluate_model(self.test_data)\n",
    "    \n",
    "    def aggeragate(self):\n",
    "        self.server.aggeragate_model(self.clients)\n",
    "    \n",
    "    def run(self, epochs: int, client_epochs: int):\n",
    "        self._init_data(self.client_num)\n",
    "        for client in self.clients:\n",
    "            client.set_training_mode(for_gradient=False)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(self.get_validation_result())\n",
    "            for client, client_train_data in self.shuffled_data(to_shuffle=False):\n",
    "                client.train(client_train_data, client_epochs=client_epochs)\n",
    "\n",
    "            self.aggeragate()\n",
    "\n",
    "            self.distribute_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0.0978\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.0984\n",
      "0.1023\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.1025\n",
      "0.0998\n",
      "0.1001\n",
      "0.1025\n",
      "0.0985\n",
      "0.0998\n",
      "0.0998\n",
      "0.0998\n",
      "0.1065\n",
      "0.1168\n",
      "0.0999\n",
      "0.0998\n",
      "0.1035\n",
      "0.0998\n",
      "0.0998\n",
      "0.0998\n",
      "0.0998\n",
      "0.0998\n",
      "0.0997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m EXPERIMENT \u001b[39m=\u001b[39m DemoCifar10Experiment(client_num\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m      2\u001b[0m                                 lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                 max_norm\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                 sampling_rate\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                 noise_generator\u001b[39m=\u001b[39mNoNoiseGenerator())\n\u001b[1;32m----> 6\u001b[0m EXPERIMENT\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 63\u001b[0m, in \u001b[0;36mDemoCifar10Experiment.run\u001b[1;34m(self, epochs, client_epochs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_validation_result())\n\u001b[0;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m client, client_train_data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffled_data(to_shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 63\u001b[0m     client\u001b[39m.\u001b[39;49mtrain(client_train_data, client_epochs\u001b[39m=\u001b[39;49mclient_epochs)\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggeragate()\n\u001b[0;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_model()\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\DifferentialPrivacy\\client\\base_client.py:74\u001b[0m, in \u001b[0;36mBaseClient.train_for_model\u001b[1;34m(self, dataset, client_epochs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(client_epochs):\n\u001b[0;32m     73\u001b[0m     sample_dataloder \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataset)\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_dpsgd(sample_dataloder, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnoise_generator)\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\DifferentialPrivacy\\models\\base_model.py:29\u001b[0m, in \u001b[0;36mBaseModel.train_dpsgd\u001b[1;34m(self, data_loader, noise_generator)\u001b[0m\n\u001b[0;32m     25\u001b[0m summed_clipped_grads \u001b[39m=\u001b[39m {name: torch\u001b[39m.\u001b[39mzeros_like(param)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     26\u001b[0m                          \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters()}\n\u001b[0;32m     28\u001b[0m \u001b[39m# Go through each batch of data\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfor\u001b[39;00m batch_x, batch_y \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m     30\u001b[0m     batch_x, batch_y \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), batch_y\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     31\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(batch_x)\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torchvision\\transforms\\transforms.py:954\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    953\u001b[0m i, j, h, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mratio)\n\u001b[1;32m--> 954\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresized_crop(img, i, j, h, w, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation)\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torchvision\\transforms\\functional.py:583\u001b[0m, in \u001b[0;36mresized_crop\u001b[1;34m(img, top, left, height, width, size, interpolation)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    582\u001b[0m     _log_api_usage_once(resized_crop)\n\u001b[1;32m--> 583\u001b[0m img \u001b[39m=\u001b[39m crop(img, top, left, height, width)\n\u001b[0;32m    584\u001b[0m img \u001b[39m=\u001b[39m resize(img, size, interpolation)\n\u001b[0;32m    585\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torchvision\\transforms\\functional.py:504\u001b[0m, in \u001b[0;36mcrop\u001b[1;34m(img, top, left, height, width)\u001b[0m\n\u001b[0;32m    502\u001b[0m     _log_api_usage_once(crop)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mcrop(img, top, left, height, width)\n\u001b[0;32m    506\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mcrop(img, top, left, height, width)\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:237\u001b[0m, in \u001b[0;36mcrop\u001b[1;34m(img, top, left, height, width)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mcrop((left, top, left \u001b[39m+\u001b[39;49m width, top \u001b[39m+\u001b[39;49m height))\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\PIL\\Image.py:1234\u001b[0m, in \u001b[0;36mImage.crop\u001b[1;34m(self, box)\u001b[0m\n\u001b[0;32m   1231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[1;32m-> 1234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_crop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim, box))\n",
      "File \u001b[1;32mc:\\Users\\shf22003\\Anaconda3\\envs\\opacus_test\\lib\\site-packages\\PIL\\Image.py:1254\u001b[0m, in \u001b[0;36mImage._crop\u001b[1;34m(self, im, box)\u001b[0m\n\u001b[0;32m   1250\u001b[0m absolute_values \u001b[39m=\u001b[39m (\u001b[39mabs\u001b[39m(x1 \u001b[39m-\u001b[39m x0), \u001b[39mabs\u001b[39m(y1 \u001b[39m-\u001b[39m y0))\n\u001b[0;32m   1252\u001b[0m _decompression_bomb_check(absolute_values)\n\u001b[1;32m-> 1254\u001b[0m \u001b[39mreturn\u001b[39;00m im\u001b[39m.\u001b[39;49mcrop((x0, y0, x1, y1))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EXPERIMENT = DemoCifar10Experiment(client_num=1,\n",
    "                                lr = 0.001, \n",
    "                                max_norm=1000,\n",
    "                                sampling_rate=0.05,\n",
    "                                noise_generator=NoNoiseGenerator())\n",
    "EXPERIMENT.run(500, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Data preprocessing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m      3\u001b[0m     [transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[0;32m      4\u001b[0m      transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m), (\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m))])\n\u001b[0;32m      6\u001b[0m \u001b[39m# Load dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load dataset\n",
    "batch_size = 32\n",
    "# data_manager = Cifar10DatasetManager(n_parties=1, \n",
    "#                                              sampling_lot_rate=0.01)\n",
    "# trainloader = data_manager.train_loaders\n",
    "# testset = data_manager.validation_loader\n",
    "# testloader = data_manager.test_loader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1aa898b1640>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object Cifar10DatasetManager.create_sampling_dataloader at 0x000001A9FAC96350>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m trainset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCIFAR10(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      2\u001b[0m                                          download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n\u001b[1;32m----> 3\u001b[0m trainloader \u001b[39m=\u001b[39m Cifar10DatasetManager(trainset, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      4\u001b[0m                                            shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'shuffle'"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "trainloader = Cifar10DatasetManager(trainset, batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     13\u001b[0m     inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice), labels\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     15\u001b[0m     model\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Initialize model and optimizer\n",
    "learning_rate = 0.001\n",
    "model = ResNet(lr=learning_rate)\n",
    "model = model.to(model.device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = model.loss_fn(outputs, labels).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), model.max_norm)\n",
    "        model.optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:  # Print average loss every 200 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cifar10_resnet.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200] loss: 2.264\n",
      "[1, 400] loss: 1.911\n",
      "[1, 600] loss: 1.715\n",
      "[1, 800] loss: 1.600\n",
      "[1, 1000] loss: 1.479\n",
      "[1, 1200] loss: 1.384\n",
      "[1, 1400] loss: 1.312\n",
      "[2, 200] loss: 1.174\n",
      "[2, 400] loss: 1.127\n",
      "[2, 600] loss: 1.084\n",
      "[2, 800] loss: 1.038\n",
      "[2, 1000] loss: 1.004\n",
      "[2, 1200] loss: 0.949\n",
      "[2, 1400] loss: 0.920\n",
      "[3, 200] loss: 0.813\n",
      "[3, 400] loss: 0.823\n",
      "[3, 600] loss: 0.793\n",
      "[3, 800] loss: 0.753\n",
      "[3, 1000] loss: 0.723\n",
      "[3, 1200] loss: 0.681\n",
      "[3, 1400] loss: 0.669\n",
      "[4, 200] loss: 0.594\n",
      "[4, 400] loss: 0.559\n",
      "[4, 600] loss: 0.577\n",
      "[4, 800] loss: 0.548\n",
      "[4, 1000] loss: 0.546\n",
      "[4, 1200] loss: 0.544\n",
      "[4, 1400] loss: 0.535\n",
      "[5, 200] loss: 0.433\n",
      "[5, 400] loss: 0.438\n",
      "[5, 600] loss: 0.426\n",
      "[5, 800] loss: 0.431\n",
      "[5, 1000] loss: 0.448\n",
      "[5, 1200] loss: 0.446\n",
      "[5, 1400] loss: 0.412\n",
      "[6, 200] loss: 0.328\n",
      "[6, 400] loss: 0.316\n",
      "[6, 600] loss: 0.349\n",
      "[6, 800] loss: 0.344\n",
      "[6, 1000] loss: 0.343\n",
      "[6, 1200] loss: 0.360\n",
      "[6, 1400] loss: 0.310\n",
      "[7, 200] loss: 0.228\n",
      "[7, 400] loss: 0.246\n",
      "[7, 600] loss: 0.255\n",
      "[7, 800] loss: 0.256\n",
      "[7, 1000] loss: 0.259\n",
      "[7, 1200] loss: 0.252\n",
      "[7, 1400] loss: 0.264\n",
      "[8, 200] loss: 0.166\n",
      "[8, 400] loss: 0.173\n",
      "[8, 600] loss: 0.182\n",
      "[8, 800] loss: 0.190\n",
      "[8, 1000] loss: 0.210\n",
      "[8, 1200] loss: 0.195\n",
      "[8, 1400] loss: 0.201\n",
      "[9, 200] loss: 0.120\n",
      "[9, 400] loss: 0.130\n",
      "[9, 600] loss: 0.142\n",
      "[9, 800] loss: 0.136\n",
      "[9, 1000] loss: 0.146\n",
      "[9, 1200] loss: 0.146\n",
      "[9, 1400] loss: 0.149\n",
      "[10, 200] loss: 0.100\n",
      "[10, 400] loss: 0.097\n",
      "[10, 600] loss: 0.106\n",
      "[10, 800] loss: 0.112\n",
      "[10, 1000] loss: 0.114\n",
      "[10, 1200] loss: 0.127\n",
      "[10, 1400] loss: 0.122\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and optimizer\n",
    "learning_rate = 0.001\n",
    "model = ResNet(lr=learning_rate)\n",
    "model = model.to(model.device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = model.loss_fn(outputs, labels).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), model.max_norm)\n",
    "        model.optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:  # Print average loss every 200 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cifar10_resnet.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 86.54%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(model.device), labels.to(model.device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
